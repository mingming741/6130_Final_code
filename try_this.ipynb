{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from random import randint, choice\n",
    "from IPython.display import clear_output\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVEnvironment():\n",
    "    \"\"\"\n",
    "    Game environment for UAV test\n",
    "    \n",
    "    ---Map---\n",
    "    \n",
    "    y-axis(length)\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "     _______________________ x-axis(width)\n",
    "     \n",
    "    Hight is a fixed value\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        # Game config\n",
    "        self.action_space = (0, 1, 2, 3) # up, right, down, left, total 4 actions\n",
    "        self.total_steps = config[\"total_steps\"] # when the game end\n",
    "        self.current_step = 0\n",
    "        if config[\"is_random_env\"] == False:\n",
    "            self.random_seed = config[\"random_seed\"]\n",
    "            random.seed(self.random_seed)\n",
    "        \n",
    "        # Map config\n",
    "        self.map = dict(width=config[\"map\"][\"width\"], length=config[\"map\"][\"length\"], height=config[\"map\"][\"height\"])\n",
    "        self.UAV_speed = config[\"UAV_speed\"]\n",
    "        self.UAV_initial_pos = config[\"UAV_initial_pos\"] \n",
    "        self.inital_state=config[\"initial_state\"]\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        self.number_of_user = config[\"number_of_user\"]\n",
    "        self.users_pos = list()\n",
    "        self.UAV_path = [] # record the path of UAV\n",
    "        for i in range(0, self.number_of_user):\n",
    "            self.users_pos.append((randint(0, self.map[\"width\"]), randint(0, self.map[\"length\"])))\n",
    "        \n",
    "        # Wireless config\n",
    "        self.g0 = config[\"wireless_parameter\"][\"g0\"]\n",
    "        self.B = config[\"wireless_parameter\"][\"B\"]\n",
    "        self.Pk = config[\"wireless_parameter\"][\"Pk\"]\n",
    "        self.noise = config[\"wireless_parameter\"][\"noise\"]\n",
    "        \n",
    "    def get_reward(self,prev_pos, UAV_pos):\n",
    "        reward = 0\n",
    "        for user_index in range(0, self.number_of_user):\n",
    "            gkm = self.g0 / (self.map[\"height\"] ** 2 + (UAV_pos[0] - self.users_pos[user_index][0]) ** 2 + (UAV_pos[1] - self.users_pos[user_index][1]) ** 2)\n",
    "            user_utility = (self.B/self.number_of_user)* math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "            reward = reward + user_utility\n",
    "        return (reward) / (10 ** 6) # Use Mkbps as signal basic unit\n",
    "    \n",
    " \n",
    "    def transition_dynamics(self, action, speed, state):\n",
    "        # given the action (direction), calculate the next state (UAV current position)\n",
    "        assert action in self.action_space\n",
    "        next_UAV_pos = list(state)\n",
    "        if action == 0:\n",
    "            # move up\n",
    "            next_UAV_pos[1] = min(next_UAV_pos[1] + speed, self.map[\"length\"])\n",
    "        if action == 1:\n",
    "            # move right\n",
    "            next_UAV_pos[0] = min(next_UAV_pos[0] + speed, self.map[\"width\"])\n",
    "        if action == 2:\n",
    "            # move down\n",
    "            next_UAV_pos[1] = max(next_UAV_pos[1] - speed, 0)\n",
    "        if action == 3:\n",
    "            # move left\n",
    "            next_UAV_pos[0] = max(next_UAV_pos[0] - speed, 0)\n",
    "        return np.array(next_UAV_pos)\n",
    "                    \n",
    "    def step(self, action, speed=-1):\n",
    "        # assume we use the max speed as the default speed, when come near to the opt-position, we can slow down the speed\n",
    "        if speed < 0 or speed >= self.UAV_speed:\n",
    "            speed = self.UAV_speed\n",
    "        \n",
    "        prev_pos=self.UAV_current_pos\n",
    "        #update pos\n",
    "        self.UAV_path.append(prev_pos)\n",
    "        self.UAV_current_pos = self.transition_dynamics(action, speed, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        state=self.UAV_current_pos/1000\n",
    "        return state, self.get_reward(prev_pos, self.UAV_current_pos), done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        return choice(self.action_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        return self.UAV_current_pos\n",
    "        \n",
    "    def print_attribute(self):\n",
    "        attrs = vars(self)\n",
    "        print(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "        \n",
    "    def print_locations(self):\n",
    "        print(\"UAV position is: {}\".format(self.UAV_current_pos))\n",
    "        print(\"Users position are: {}\".format(self.users_pos))\n",
    "        \n",
    "    def print_map(self):\n",
    "        x_list = [pos[0] for pos in self.users_pos]\n",
    "        y_list = [pos[1] for pos in self.users_pos]\n",
    "        x_list.append(self.UAV_current_pos[0])\n",
    "        y_list.append(self.UAV_current_pos[1])\n",
    "        for i in range(0, len(self.UAV_path)):\n",
    "            x_list.append(self.UAV_path[i][0])\n",
    "            y_list.append(self.UAV_path[i][1])\n",
    "        \n",
    "        colors = np.array([\"red\", \"green\", \"blue\"])\n",
    "        sizes = []\n",
    "        colors_map = []\n",
    "        for i in range(0, self.number_of_user):\n",
    "            sizes.append(25)\n",
    "            colors_map.append(1)\n",
    "        sizes.append(50)\n",
    "        colors_map.append(0)\n",
    "        for i in range(0, len(self.UAV_path)):\n",
    "            sizes.append(10)\n",
    "            colors_map.append(2)\n",
    "        for i in range(0, len(self.UAV_path) - 1):\n",
    "            x_values = [self.UAV_path[i][0], self.UAV_path[i+1][0]]\n",
    "            y_values = [self.UAV_path[i][1], self.UAV_path[i+1][1]]\n",
    "            plt.plot(x_values, y_values, 'b')\n",
    "        plt.scatter(x_list, y_list, c=colors[colors_map], s=sizes) \n",
    "        plt.axis([0, self.map[\"width\"], 0, self.map[\"length\"]])\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "environment_config = dict(\n",
    "    total_steps = 50,\n",
    "    random_seed = 10,\n",
    "    is_random_env = False,\n",
    "    map=dict(\n",
    "        width=1000,\n",
    "        length=1000,\n",
    "        height=300\n",
    "    ),\n",
    "    number_of_user = 10,\n",
    "    UAV_speed = 50,\n",
    "    UAV_initial_pos = np.array([0, 0]),\n",
    "    initial_state=0,\n",
    "    wireless_parameter = dict(\n",
    "        g0 = 10 ** (-5),\n",
    "        B = 10 ** (6),\n",
    "        Pk = 0.1,\n",
    "        noise = 10 ** (-9)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate(policy, num_episodes=1, render=False):\n",
    "    global environment_config\n",
    "    env = UAVEnvironment(environment_config)\n",
    "    rewards = []\n",
    "    if render: num_episodes = 1\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(act)\n",
    "            act = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                env.print_attribute()\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Action: {}\".format(act))\n",
    "                print(\"UAV current position x: {}, y: {}\".format(env.UAV_current_pos[0], env.UAV_current_pos[1]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    assert inspect.isclass(trainer_cls)\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    trainer = trainer_cls(config)\n",
    "    config = trainer.config\n",
    "    start = now = time.time()\n",
    "    stats = []\n",
    "    for i in range(config['max_iteration'] + 1):\n",
    "        stat = trainer.train()\n",
    "        stats.append(stat or {})\n",
    "        if i % config['evaluate_interval'] == 0 or \\\n",
    "                i == config[\"max_iteration\"]:\n",
    "            reward = trainer.evaluate(config.get(\"evaluate_num_episodes\", 50))\n",
    "            print(\"({:.1f}s,+{:.1f}s)\\tIteration {}, current mean episode \"\n",
    "                  \"reward is {}. {}\".format(\n",
    "                time.time() - start, time.time() - now, i, reward,\n",
    "                {k: round(np.mean(v), 4) for k, v in\n",
    "                 stat.items()} if stat else \"\"))\n",
    "            now = time.time()\n",
    "        if reward_threshold is not None and reward > reward_threshold:\n",
    "            print(\"In {} iteration, current mean episode reward {:.3f} is \"\n",
    "                  \"greater than reward threshold {}. Congratulation! Now we \"\n",
    "                  \"exit the training process.\".format(\n",
    "                i, reward, reward_threshold))\n",
    "            break\n",
    "    return trainer, stats\n",
    "\n",
    "\n",
    "def to_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.float32)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x\n",
    "\n",
    "def to_long_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.long)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x\n",
    "\n",
    "class ExperienceReplayMemory:\n",
    "    \"\"\"Store and sample the transitions\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class PytorchModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(PytorchModel, self).__init__()\n",
    "        self.action_value = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_dim,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,act_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.action_value(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_config = dict(\n",
    "    max_iteration=1000,\n",
    "    max_episode_length=1000,\n",
    "    evaluate_interval=100,\n",
    "    gamma=0.99,\n",
    "    eps=0.3,\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    hidden_dim=100,\n",
    "    clip_norm=1.0,\n",
    "    clip_gradient=True,\n",
    "    memory_size=50000,\n",
    "    learn_start=5000,\n",
    "    batch_size=32,\n",
    "    target_update_freq=500,  # in steps\n",
    "    learn_freq=1,  # in steps\n",
    "    n=1\n",
    ")\n",
    "\n",
    "class DQNTrainer():\n",
    "    def __init__(self, config):\n",
    "        self.config = merge_config(config, pytorch_config)\n",
    "        self.env = UAVEnvironment(config)\n",
    "        self.act_dim=4\n",
    "        self.obs_dim=2 \n",
    "        self.eps = self.config['eps']\n",
    "        self.hidden_dim = self.config[\"hidden_dim\"]\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.n = self.config[\"n\"]\n",
    "        self.initialize_parameters()\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.learn_start = self.config[\"learn_start\"]\n",
    "        self.batch_size = self.config[\"batch_size\"]\n",
    "        self.target_update_freq = self.config[\"target_update_freq\"]\n",
    "        self.clip_norm = self.config[\"clip_norm\"]\n",
    "        self.step_since_update = 0\n",
    "        self.total_step = 0\n",
    "        self.memory = ExperienceReplayMemory(self.config[\"memory_size\"])\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        self.network = PytorchModel(self.obs_dim,self.act_dim)\n",
    "        self.network.eval()\n",
    "        self.network.share_memory()\n",
    "        self.target_network = PytorchModel(self.obs_dim,self.act_dim)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.network.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        values=self.network(processed_state)\n",
    "        return values.data.numpy()\n",
    "\n",
    "    def train(self):\n",
    "        s = self.env.reset()\n",
    "        processed_s = self.process_state(s)\n",
    "        act = self.compute_action(processed_s)\n",
    "        stat = {\"loss\": []}\n",
    "        for t in range(self.max_episode_length):\n",
    "            next_state, reward, done = self.env.step(act)\n",
    "            next_processed_s = self.process_state(next_state)\n",
    "            self.memory.push((processed_s, act, reward, next_processed_s, done))\n",
    "            processed_s = next_processed_s\n",
    "            act = self.compute_action(next_processed_s)\n",
    "            self.step_since_update += 1\n",
    "            self.total_step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            if t % self.config[\"learn_freq\"] != 0:\n",
    "                continue\n",
    "            if len(self.memory) < self.learn_start:\n",
    "                continue\n",
    "            elif len(self.memory) == self.learn_start:\n",
    "                print(\"Current memory contains {} transitions, \"\n",
    "                      \"start learning!\".format(self.learn_start))\n",
    "\n",
    "            batch = self.memory.sample(self.batch_size)\n",
    "            state_batch = to_tensor( np.stack([transition[0] for transition in batch]))\n",
    "            action_batch = to_long_tensor(np.stack([transition[1] for transition in batch]))\n",
    "            reward_batch = to_tensor(np.stack([transition[2] for transition in batch]))\n",
    "            next_state_batch = torch.stack([transition[3] for transition in batch])\n",
    "            done_batch = to_tensor(np.stack([transition[4] for transition in batch]))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_state_values=self.target_network(next_state_batch)\n",
    "                Q_t_plus_one = next_state_values.max(1)[0].detach()             \n",
    "                assert isinstance(Q_t_plus_one, torch.Tensor)\n",
    "                assert Q_t_plus_one.dim() == 1\n",
    "                Q_target = reward_batch+(1-done_batch)*Q_t_plus_one\n",
    "            self.network.train()\n",
    "            \n",
    "            state_action_values=self.network(state_batch)\n",
    "            Q_t = torch.t(state_action_values).gather(0,action_batch)\n",
    "            assert Q_t.shape == Q_target.shape\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss(input=Q_t, target=Q_target)\n",
    "            loss_value = loss.item()\n",
    "            stat['loss'].append(loss_value)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.network.parameters(), self.clip_norm)   \n",
    "            self.optimizer.step()\n",
    "            self.network.eval()\n",
    "\n",
    "        if len(self.memory) >= self.learn_start and self.step_since_update > self.target_update_freq:\n",
    "            self.step_since_update = 0\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "            self.target_network.eval()\n",
    "        return {\"loss\": np.mean(stat[\"loss\"]), \"episode_len\": t}\n",
    "    \n",
    "    def compute_action(self, processed_state, eps=None):\n",
    "        values = self.compute_values(processed_state)\n",
    "        assert values.ndim == 1, values.shape\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "        if np.random.uniform(0,1)< eps:\n",
    "            action=self.env.action_sample()\n",
    "        else:\n",
    "            action=np.argmax(values)\n",
    "        return action\n",
    "\n",
    "    def compute_activation(self, processed_state):\n",
    "        activation = np.dot(self.hidden_parameters.T,processed_state)\n",
    "        return activation\n",
    "\n",
    "    def compute_gradient(self, processed_states, actions, rewards, tau, T):\n",
    "        n = self.n\n",
    "        G = 0\n",
    "        for i in range(tau+1,min(T,tau+n)+1):\n",
    "            G+=np.power(self.gamma,i-tau-1)*rewards[i]\n",
    "        if tau + n < T:\n",
    "            Q_tau_plus_n = self.compute_values(processed_states[tau+n])\n",
    "            act=actions[tau+n]  \n",
    "            G = G + (self.gamma ** n) * Q_tau_plus_n[act]\n",
    "        cur_state = processed_states[tau]\n",
    "        loss_grad = np.zeros((self.act_dim, 1))  # [act_dim, 1]\n",
    "        q=self.compute_values(cur_state)\n",
    "        act=actions[tau]\n",
    "        loss_grad[act]=-(G-q[act])\n",
    "\n",
    "        activation=self.compute_activation(cur_state)\n",
    "        activation=np.array([activation])\n",
    "        output_gradient = np.dot(activation.T,loss_grad.T)\n",
    "  \n",
    "        assert np.all(np.isfinite(output_gradient)), \\\n",
    "            \"Invalid value occurs in output_gradient! {}\".format(\n",
    "                output_gradient)\n",
    "        assert np.all(np.isfinite(hidden_gradient)), \\\n",
    "            \"Invalid value occurs in hidden_gradient! {}\".format(\n",
    "                hidden_gradient)\n",
    "        return [hidden_gradient, output_gradient]\n",
    "\n",
    "    def apply_gradient(self, gradients):\n",
    "        \"\"\"Apply the gradientss to the two layers' parameters.\"\"\"\n",
    "        assert len(gradients) == 2\n",
    "        hidden_gradient, output_gradient = gradients\n",
    "        assert output_gradient.shape == (self.hidden_dim, self.act_dim)\n",
    "        assert hidden_gradient.shape == (self.obs_dim, self.hidden_dim)\n",
    "        if self.config[\"clip_gradient\"]:\n",
    "            clip_norm = self.config[\"clip_norm\"]\n",
    "            output_gradient=output_gradient*clip_norm/max(clip_norm, np.linalg.norm(output_gradient))\n",
    "            hidden_gradient=hidden_gradient*clip_norm/max(clip_norm, np.linalg.norm(hidden_gradient))\n",
    "\n",
    "        self.output_parameters-= self.learning_rate*output_gradient\n",
    "        self.hidden_parameters-= self.learning_rate*hidden_gradient\n",
    "\n",
    "    def evaluate(self, num_episodes=50, *args, **kwargs):\n",
    "        policy = lambda raw_state: self.compute_action(\n",
    "            self.process_state(raw_state), eps=0.0)\n",
    "        result = evaluate(policy, num_episodes, *args, **kwargs)\n",
    "        return result\n",
    "    \n",
    "    def process_state(self, state):\n",
    "        return torch.from_numpy(state).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5s,+0.5s)\tIteration 0, current mean episode reward is 0.1947482809852421. {'loss': nan, 'episode_len': 49.0}\n",
      "(12.8s,+12.3s)\tIteration 100, current mean episode reward is 0.16357544228636653. {'loss': 0.0004, 'episode_len': 49.0}\n",
      "(29.5s,+16.7s)\tIteration 200, current mean episode reward is 0.1947482809852421. {'loss': 0.0006, 'episode_len': 49.0}\n",
      "(51.1s,+21.6s)\tIteration 300, current mean episode reward is 0.1947482809852421. {'loss': 0.0008, 'episode_len': 49.0}\n",
      "(65.4s,+14.3s)\tIteration 400, current mean episode reward is 0.2062009020572017. {'loss': 0.0009, 'episode_len': 49.0}\n",
      "(82.8s,+17.4s)\tIteration 500, current mean episode reward is 0.1947482809852421. {'loss': 0.0009, 'episode_len': 49.0}\n",
      "(99.5s,+16.7s)\tIteration 600, current mean episode reward is 0.3334058177332664. {'loss': 0.001, 'episode_len': 49.0}\n",
      "(116.1s,+16.5s)\tIteration 700, current mean episode reward is 0.1947482809852421. {'loss': 0.0016, 'episode_len': 49.0}\n",
      "(130.9s,+14.8s)\tIteration 800, current mean episode reward is 0.1947482809852421. {'loss': 0.002, 'episode_len': 49.0}\n",
      "(147.8s,+16.9s)\tIteration 900, current mean episode reward is 0.1947482809852421. {'loss': 0.0018, 'episode_len': 49.0}\n",
      "(163.1s,+15.3s)\tIteration 1000, current mean episode reward is 0.2551598532986239. {'loss': 0.0027, 'episode_len': 49.0}\n"
     ]
    }
   ],
   "source": [
    "config=merge_config(environment_config, dict(\n",
    "    max_iteration=1000,\n",
    "    evaluate_interval=100, \n",
    "    learning_rate=0.0001,\n",
    "    clip_norm=10.0,\n",
    "    memory_size=50000,\n",
    "    learn_start=1000,\n",
    "    eps=0.03,\n",
    "    target_update_freq=1000,\n",
    "    batch_size=32,\n",
    "))\n",
    "pytorch_trainer, pytorch_stat = run(DQNTrainer, config,reward_threshold=5) #,reward_threshold=2\n",
    "\n",
    "reward = pytorch_trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: (0, 1, 2, 3), total_steps: 50, current_step: 50, random_seed: 10, map: {'width': 1000, 'length': 1000, 'height': 300}, UAV_speed: 50, UAV_initial_pos: [0 0], inital_state: 0, UAV_current_pos: [1000  300], number_of_user: 10, users_pos: [(585, 33), (439, 494), (591, 15), (211, 473), (832, 503), (843, 284), (669, 830), (164, 35), (533, 501), (335, 77)], UAV_path: [array([0, 0]), array([50,  0]), array([100,   0]), array([150,   0]), array([200,   0]), array([250,   0]), array([300,   0]), array([350,   0]), array([400,   0]), array([450,   0]), array([500,   0]), array([550,   0]), array([600,   0]), array([650,   0]), array([700,   0]), array([750,   0]), array([800,   0]), array([850,   0]), array([850,  50]), array([900,  50]), array([900, 100]), array([950, 100]), array([950, 150]), array([1000,  150]), array([1000,  200]), array([1000,  250]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300]), array([1000,  300])], g0: 1e-05, B: 1000000, Pk: 0.1, noise: 1e-09\n",
      "Current Step: 50\n",
      "Action: 1\n",
      "UAV current position x: 1000, y: 300\n",
      "Current step reward: 0.004658510551390299, episodes rewards: 0.2551598532986239\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward for your Pytorch agent:  0.2551598532986239\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Average episode reward for your Pytorch agent: \",\n",
    "      pytorch_trainer.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
