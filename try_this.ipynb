{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN implementation\n",
    "This is the Deep Q-learning solution for UAV environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from utils import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from random import randint, choice\n",
    "from IPython.display import clear_output\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVEnvironment():\n",
    "    \"\"\"\n",
    "    Game environment for UAV test\n",
    "    \n",
    "    ---Map---\n",
    "    \n",
    "    y-axis(length)\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "     _______________________ x-axis(width)\n",
    "     \n",
    "    Hight is a fixed value\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        # Game config\n",
    "        self.action_space = (0, 1, 2, 3) # up, right, down, left, total 4 actions\n",
    "        self.total_steps = config[\"total_steps\"] # when the game end\n",
    "        self.current_step = 0\n",
    "        if config[\"is_random_env\"] == False:\n",
    "            self.random_seed = config[\"random_seed\"]\n",
    "            random.seed(self.random_seed)\n",
    "        \n",
    "        # Map config\n",
    "        self.map = dict(width=config[\"map\"][\"width\"], length=config[\"map\"][\"length\"], height=config[\"map\"][\"height\"])\n",
    "        self.UAV_speed = config[\"UAV_speed\"]\n",
    "        self.UAV_initial_pos = config[\"UAV_initial_pos\"] \n",
    "        self.inital_state=config[\"initial_state\"]\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        self.number_of_user = config[\"number_of_user\"]\n",
    "        self.users_pos = list()\n",
    "        self.UAV_path = [] # record the path of UAV\n",
    "        for i in range(0, self.number_of_user):\n",
    "            self.users_pos.append((randint(0, self.map[\"width\"]), randint(0, self.map[\"length\"])))\n",
    "        \n",
    "        # Wireless config\n",
    "        self.g0 = config[\"wireless_parameter\"][\"g0\"]\n",
    "        self.B = config[\"wireless_parameter\"][\"B\"]\n",
    "        self.Pk = config[\"wireless_parameter\"][\"Pk\"]\n",
    "        self.noise = config[\"wireless_parameter\"][\"noise\"]\n",
    "        \n",
    "    def get_reward(self,prev_pos, UAV_pos):\n",
    "        reward = 0\n",
    "        for user_index in range(0, self.number_of_user):\n",
    "            gkm = self.g0 / (self.map[\"height\"] ** 2 + (UAV_pos[0] - self.users_pos[user_index][0]) ** 2 + (UAV_pos[1] - self.users_pos[user_index][1]) ** 2)\n",
    "            user_utility = (self.B/self.number_of_user)* math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "            reward = reward + user_utility\n",
    "        return (reward) / (10 ** 6) # Use Mkbps as signal basic unit\n",
    "    \n",
    " \n",
    "    def transition_dynamics(self, action, speed, state):\n",
    "        # given the action (direction), calculate the next state (UAV current position)\n",
    "        assert action in self.action_space\n",
    "        next_UAV_pos = list(state)\n",
    "        if action == 0:\n",
    "            # move up\n",
    "            next_UAV_pos[1] = min(next_UAV_pos[1] + speed, self.map[\"length\"])\n",
    "        if action == 1:\n",
    "            # move right\n",
    "            next_UAV_pos[0] = min(next_UAV_pos[0] + speed, self.map[\"width\"])\n",
    "        if action == 2:\n",
    "            # move down\n",
    "            next_UAV_pos[1] = max(next_UAV_pos[1] - speed, 0)\n",
    "        if action == 3:\n",
    "            # move left\n",
    "            next_UAV_pos[0] = max(next_UAV_pos[0] - speed, 0)\n",
    "        return np.array(next_UAV_pos)\n",
    "                    \n",
    "    def step(self, action, speed=-1):\n",
    "        # assume we use the max speed as the default speed, when come near to the opt-position, we can slow down the speed\n",
    "        if speed < 0 or speed >= self.UAV_speed:\n",
    "            speed = self.UAV_speed\n",
    "        \n",
    "        prev_pos=self.UAV_current_pos\n",
    "        #update pos\n",
    "        self.UAV_path.append(prev_pos)\n",
    "        self.UAV_current_pos = self.transition_dynamics(action, speed, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        state=self.UAV_current_pos/1000\n",
    "        return state, self.get_reward(prev_pos, self.UAV_current_pos), done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        return choice(self.action_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        return self.UAV_current_pos\n",
    "        \n",
    "    def print_attribute(self):\n",
    "        attrs = vars(self)\n",
    "        print(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "        \n",
    "    def print_locations(self):\n",
    "        print(\"UAV position is: {}\".format(self.UAV_current_pos))\n",
    "        print(\"Users position are: {}\".format(self.users_pos))\n",
    "        \n",
    "    def print_map(self):\n",
    "        x_list = [pos[0] for pos in self.users_pos]\n",
    "        y_list = [pos[1] for pos in self.users_pos]\n",
    "        x_list.append(self.UAV_current_pos[0])\n",
    "        y_list.append(self.UAV_current_pos[1])\n",
    "        for i in range(0, len(self.UAV_path)):\n",
    "            x_list.append(self.UAV_path[i][0])\n",
    "            y_list.append(self.UAV_path[i][1])\n",
    "        \n",
    "        colors = np.array([\"red\", \"green\", \"blue\"])\n",
    "        sizes = []\n",
    "        colors_map = []\n",
    "        for i in range(0, self.number_of_user):\n",
    "            sizes.append(25)\n",
    "            colors_map.append(1)\n",
    "        sizes.append(50)\n",
    "        colors_map.append(0)\n",
    "        for i in range(0, len(self.UAV_path)):\n",
    "            sizes.append(10)\n",
    "            colors_map.append(2)\n",
    "        for i in range(0, len(self.UAV_path) - 1):\n",
    "            x_values = [self.UAV_path[i][0], self.UAV_path[i+1][0]]\n",
    "            y_values = [self.UAV_path[i][1], self.UAV_path[i+1][1]]\n",
    "            plt.plot(x_values, y_values, 'b')\n",
    "        plt.scatter(x_list, y_list, c=colors[colors_map], s=sizes) \n",
    "        plt.axis([0, self.map[\"width\"], 0, self.map[\"length\"]])\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "environment_config = dict(\n",
    "    total_steps = 50,\n",
    "    random_seed = 10,\n",
    "    is_random_env = False,\n",
    "    map=dict(\n",
    "        width=1000,\n",
    "        length=1000,\n",
    "        height=300\n",
    "    ),\n",
    "    number_of_user = 10,\n",
    "    UAV_speed = 50,\n",
    "    UAV_initial_pos = np.array([0, 0]),\n",
    "    initial_state=0,\n",
    "    wireless_parameter = dict(\n",
    "        g0 = 10 ** (-5),\n",
    "        B = 10 ** (6),\n",
    "        Pk = 0.1,\n",
    "        noise = 10 ** (-9)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate(policy, num_episodes=1, render=False):\n",
    "    global environment_config\n",
    "    env = UAVEnvironment(environment_config)\n",
    "    rewards = []\n",
    "    if render: num_episodes = 1\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(act)\n",
    "            act = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                env.print_attribute()\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Action: {}\".format(act))\n",
    "                print(\"UAV current position x: {}, y: {}\".format(env.UAV_current_pos[0], env.UAV_current_pos[1]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    assert inspect.isclass(trainer_cls)\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    trainer = trainer_cls(config)\n",
    "    config = trainer.config\n",
    "    start = now = time.time()\n",
    "    stats = []\n",
    "    for i in range(config['max_iteration'] + 1):\n",
    "        stat = trainer.train()\n",
    "        stats.append(stat or {})\n",
    "        if i % config['evaluate_interval'] == 0 or \\\n",
    "                i == config[\"max_iteration\"]:\n",
    "            reward = trainer.evaluate(config.get(\"evaluate_num_episodes\", 50))\n",
    "            print(\"({:.1f}s,+{:.1f}s)\\tIteration {}, current mean episode \"\n",
    "                  \"reward is {}. {}\".format(\n",
    "                time.time() - start, time.time() - now, i, reward,\n",
    "                {k: round(np.mean(v), 4) for k, v in\n",
    "                 stat.items()} if stat else \"\"))\n",
    "            now = time.time()\n",
    "        if reward_threshold is not None and reward > reward_threshold:\n",
    "            print(\"In {} iteration, current mean episode reward {:.3f} is \"\n",
    "                  \"greater than reward threshold {}. Congratulation! Now we \"\n",
    "                  \"exit the training process.\".format(\n",
    "                i, reward, reward_threshold))\n",
    "            break\n",
    "    return trainer, stats\n",
    "\n",
    "\n",
    "def to_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.float32)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x\n",
    "\n",
    "def to_long_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.long)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x\n",
    "\n",
    "class ExperienceReplayMemory:\n",
    "    \"\"\"Store and sample the transitions\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class PytorchModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(PytorchModel, self).__init__()\n",
    "        self.action_value = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_dim,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,act_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.action_value(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_config = dict(\n",
    "    max_iteration=1000,\n",
    "    max_episode_length=1000,\n",
    "    evaluate_interval=100,\n",
    "    gamma=0.99,\n",
    "    eps=0.3,\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    hidden_dim=100,\n",
    "    clip_norm=1.0,\n",
    "    clip_gradient=True,\n",
    "    memory_size=50000,\n",
    "    learn_start=5000,\n",
    "    batch_size=32,\n",
    "    target_update_freq=500,  # in steps\n",
    "    learn_freq=1,  # in steps\n",
    "    n=1\n",
    ")\n",
    "\n",
    "class DQNTrainer():\n",
    "    def __init__(self, config):\n",
    "        self.config = merge_config(config, pytorch_config)\n",
    "        self.env = UAVEnvironment(config)\n",
    "        self.act_dim=4\n",
    "        self.obs_dim=2 \n",
    "        self.eps = self.config['eps']\n",
    "        self.hidden_dim = self.config[\"hidden_dim\"]\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.n = self.config[\"n\"]\n",
    "        self.initialize_parameters()\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.learn_start = self.config[\"learn_start\"]\n",
    "        self.batch_size = self.config[\"batch_size\"]\n",
    "        self.target_update_freq = self.config[\"target_update_freq\"]\n",
    "        self.clip_norm = self.config[\"clip_norm\"]\n",
    "        self.step_since_update = 0\n",
    "        self.total_step = 0\n",
    "        self.memory = ExperienceReplayMemory(self.config[\"memory_size\"])\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        self.network = PytorchModel(self.obs_dim,self.act_dim)\n",
    "        self.network.eval()\n",
    "        self.network.share_memory()\n",
    "        self.target_network = PytorchModel(self.obs_dim,self.act_dim)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.network.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        values=self.network(processed_state)\n",
    "        return values.data.numpy()\n",
    "\n",
    "    def train(self):\n",
    "        s = self.env.reset()\n",
    "        processed_s = self.process_state(s)\n",
    "        act = self.compute_action(processed_s)\n",
    "        stat = {\"loss\": []}\n",
    "        for t in range(self.max_episode_length):\n",
    "            next_state, reward, done = self.env.step(act)\n",
    "            next_processed_s = self.process_state(next_state)\n",
    "            self.memory.push((processed_s, act, reward, next_processed_s, done))\n",
    "            processed_s = next_processed_s\n",
    "            act = self.compute_action(next_processed_s)\n",
    "            self.step_since_update += 1\n",
    "            self.total_step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            if t % self.config[\"learn_freq\"] != 0:\n",
    "                continue\n",
    "            if len(self.memory) < self.learn_start:\n",
    "                continue\n",
    "            elif len(self.memory) == self.learn_start:\n",
    "                print(\"Current memory contains {} transitions, \"\n",
    "                      \"start learning!\".format(self.learn_start))\n",
    "\n",
    "            batch = self.memory.sample(self.batch_size)\n",
    "            state_batch = to_tensor( np.stack([transition[0] for transition in batch]))\n",
    "            action_batch = to_long_tensor(np.stack([transition[1] for transition in batch]))\n",
    "            reward_batch = to_tensor(np.stack([transition[2] for transition in batch]))\n",
    "            next_state_batch = torch.stack([transition[3] for transition in batch])\n",
    "            done_batch = to_tensor(np.stack([transition[4] for transition in batch]))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_state_values=self.target_network(next_state_batch)\n",
    "                Q_t_plus_one = next_state_values.max(1)[0].detach()             \n",
    "                assert isinstance(Q_t_plus_one, torch.Tensor)\n",
    "                assert Q_t_plus_one.dim() == 1\n",
    "                Q_target = reward_batch+(1-done_batch)*Q_t_plus_one\n",
    "            self.network.train()\n",
    "            \n",
    "            state_action_values=self.network(state_batch)\n",
    "            Q_t = torch.t(state_action_values).gather(0,action_batch)\n",
    "            assert Q_t.shape == Q_target.shape\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss(input=Q_t, target=Q_target)\n",
    "            loss_value = loss.item()\n",
    "            stat['loss'].append(loss_value)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.network.parameters(), self.clip_norm)   \n",
    "            self.optimizer.step()\n",
    "            self.network.eval()\n",
    "\n",
    "        if len(self.memory) >= self.learn_start and self.step_since_update > self.target_update_freq:\n",
    "            self.step_since_update = 0\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "            self.target_network.eval()\n",
    "        return {\"loss\": np.mean(stat[\"loss\"]), \"episode_len\": t}\n",
    "    \n",
    "    def compute_action(self, processed_state, eps=None):\n",
    "        values = self.compute_values(processed_state)\n",
    "        assert values.ndim == 1, values.shape\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "        if np.random.uniform(0,1)< eps:\n",
    "            action=self.env.action_sample()\n",
    "        else:\n",
    "            action=np.argmax(values)\n",
    "        return action\n",
    "\n",
    "    def compute_activation(self, processed_state):\n",
    "        activation = np.dot(self.hidden_parameters.T,processed_state)\n",
    "        return activation\n",
    "\n",
    "    def compute_gradient(self, processed_states, actions, rewards, tau, T):\n",
    "        n = self.n\n",
    "        G = 0\n",
    "        for i in range(tau+1,min(T,tau+n)+1):\n",
    "            G+=np.power(self.gamma,i-tau-1)*rewards[i]\n",
    "        if tau + n < T:\n",
    "            Q_tau_plus_n = self.compute_values(processed_states[tau+n])\n",
    "            act=actions[tau+n]  \n",
    "            G = G + (self.gamma ** n) * Q_tau_plus_n[act]\n",
    "        cur_state = processed_states[tau]\n",
    "        loss_grad = np.zeros((self.act_dim, 1))  # [act_dim, 1]\n",
    "        q=self.compute_values(cur_state)\n",
    "        act=actions[tau]\n",
    "        loss_grad[act]=-(G-q[act])\n",
    "\n",
    "        activation=self.compute_activation(cur_state)\n",
    "        activation=np.array([activation])\n",
    "        output_gradient = np.dot(activation.T,loss_grad.T)\n",
    "  \n",
    "        assert np.all(np.isfinite(output_gradient)), \\\n",
    "            \"Invalid value occurs in output_gradient! {}\".format(\n",
    "                output_gradient)\n",
    "        assert np.all(np.isfinite(hidden_gradient)), \\\n",
    "            \"Invalid value occurs in hidden_gradient! {}\".format(\n",
    "                hidden_gradient)\n",
    "        return [hidden_gradient, output_gradient]\n",
    "\n",
    "    def apply_gradient(self, gradients):\n",
    "        \"\"\"Apply the gradientss to the two layers' parameters.\"\"\"\n",
    "        assert len(gradients) == 2\n",
    "        hidden_gradient, output_gradient = gradients\n",
    "        assert output_gradient.shape == (self.hidden_dim, self.act_dim)\n",
    "        assert hidden_gradient.shape == (self.obs_dim, self.hidden_dim)\n",
    "        if self.config[\"clip_gradient\"]:\n",
    "            clip_norm = self.config[\"clip_norm\"]\n",
    "            output_gradient=output_gradient*clip_norm/max(clip_norm, np.linalg.norm(output_gradient))\n",
    "            hidden_gradient=hidden_gradient*clip_norm/max(clip_norm, np.linalg.norm(hidden_gradient))\n",
    "\n",
    "        self.output_parameters-= self.learning_rate*output_gradient\n",
    "        self.hidden_parameters-= self.learning_rate*hidden_gradient\n",
    "\n",
    "    def evaluate(self, num_episodes=50, *args, **kwargs):\n",
    "        policy = lambda raw_state: self.compute_action(\n",
    "            self.process_state(raw_state), eps=0.0)\n",
    "        result = evaluate(policy, num_episodes, *args, **kwargs)\n",
    "        return result\n",
    "    \n",
    "    def process_state(self, state):\n",
    "        return torch.from_numpy(state).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=merge_config(environment_config, dict(\n",
    "    max_iteration=1000,\n",
    "    evaluate_interval=100, \n",
    "    learning_rate=0.0001,\n",
    "    clip_norm=10.0,\n",
    "    memory_size=50000,\n",
    "    learn_start=1000,\n",
    "    eps=0.03,\n",
    "    target_update_freq=1000,\n",
    "    batch_size=32,\n",
    "))\n",
    "pytorch_trainer, pytorch_stat = run(DQNTrainer, config,reward_threshold=5) #,reward_threshold=2\n",
    "\n",
    "reward = pytorch_trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Average episode reward for your Pytorch agent: \",\n",
    "      pytorch_trainer.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
