{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "from utils import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from random import randint, choice\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVEnvironment():\n",
    "    \"\"\"\n",
    "    Game environment for UAV test\n",
    "    \n",
    "    ---Map---\n",
    "    \n",
    "    y-axis(length)\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "     _______________________ x-axis(width)\n",
    "     \n",
    "    Hight is a fixed value\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        # Game config\n",
    "        self.action_space = (0, 1, 2, 3) # up, right, down, left, total 4 actions\n",
    "        self.total_steps = config[\"total_steps\"] # when the game end\n",
    "        self.current_step = 0\n",
    "        if config[\"is_random_env\"] == False:\n",
    "            self.random_seed = config[\"random_seed\"]\n",
    "            random.seed(self.random_seed)\n",
    "        \n",
    "        # Map config\n",
    "        self.map = dict(width=config[\"map\"][\"width\"], length=config[\"map\"][\"length\"], height=config[\"map\"][\"height\"])\n",
    "        self.UAV_speed = config[\"UAV_speed\"]\n",
    "        self.UAV_initial_pos = config[\"UAV_initial_pos\"] # a tuple\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        self.number_of_user = config[\"number_of_user\"]\n",
    "        self.users_pos = list()\n",
    "        self.UAV_path = [] # record the path of UAV\n",
    "        for i in range(0, self.number_of_user):\n",
    "            self.users_pos.append((randint(0, self.map[\"width\"]), randint(0, self.map[\"length\"])))\n",
    "        \n",
    "        # Wireless config\n",
    "        self.g0 = config[\"wireless_parameter\"][\"g0\"]\n",
    "        self.B = config[\"wireless_parameter\"][\"B\"]\n",
    "        self.Pk = config[\"wireless_parameter\"][\"Pk\"]\n",
    "        self.noise = config[\"wireless_parameter\"][\"noise\"]\n",
    "        \n",
    "    def get_reward(self, UAV_pos):\n",
    "        # One step Reward is define as the summation of all user's utility\n",
    "        reward = 0\n",
    "        for user_index in range(0, self.number_of_user):\n",
    "            gkm = self.g0 / (self.map[\"height\"] ** 2 + (UAV_pos[0] - self.users_pos[user_index][0]) ** 2 + (UAV_pos[1] - self.users_pos[user_index][1]) ** 2)\n",
    "            #user_utility = self.B * math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "            user_utility = (self.B/self.number_of_user)* math.log(1 + self.Pk * gkm / self.noise, 2)\n",
    "            reward = reward + user_utility\n",
    "        return reward / (10 ** 6) # Use Mkbps as signal basic unit\n",
    "    \n",
    "    def transition_dynamics(self, action, speed, state):\n",
    "        # given the action (direction), calculate the next state (UAV current position)\n",
    "        assert action in self.action_space\n",
    "        next_UAV_pos = list(state)\n",
    "        if action == 0:\n",
    "            # move up\n",
    "            next_UAV_pos[1] = min(next_UAV_pos[1] + speed, self.map[\"length\"])\n",
    "        if action == 1:\n",
    "            # move right\n",
    "            next_UAV_pos[0] = min(next_UAV_pos[0] + speed, self.map[\"width\"])\n",
    "        if action == 2:\n",
    "            # move down\n",
    "            next_UAV_pos[1] = max(next_UAV_pos[1] - speed, 0)\n",
    "        if action == 3:\n",
    "            # move left\n",
    "            next_UAV_pos[0] = max(next_UAV_pos[0] - speed, 0)\n",
    "        return tuple(next_UAV_pos)\n",
    "    \n",
    "    def get_transition(self):\n",
    "        # This function only works for model based, we are trying to disable this function to try more algorithm\n",
    "        # Return a table of transition, we assume UAV use fixed flying speed\n",
    "        \"\"\"\n",
    "        Structure:\n",
    "        transition[\n",
    "            x_0[\n",
    "                y_0[\n",
    "                    {next_state, reward}, # for action 1\n",
    "                    {next_state, reward}, # for action 2\n",
    "                    ...\n",
    "                    {next_state, reward}, # for action 20\n",
    "                ],\n",
    "                y_1*v[],\n",
    "                ...\n",
    "                y_h-1*v[]\n",
    "            ],\n",
    "            x_1*v[],\n",
    "            x_2*v[],\n",
    "            ...\n",
    "            x_w-1*v[]\n",
    "        ]\n",
    "        \"\"\"\n",
    "        transition = list()\n",
    "        for state_x in range(0, int(self.map[\"width\"] / self.UAV_speed) + 1):\n",
    "            transition.append(list())\n",
    "            for state_y in range(0, int(self.map[\"length\"] / self.UAV_speed) + 1):\n",
    "                transition[state_x].append(list())\n",
    "                for action in self.action_space:\n",
    "                    next_state = self.transition_dynamics(action, self.UAV_speed, (state_x * self.UAV_speed, state_y * self.UAV_speed))\n",
    "                    reward = self.get_reward(next_state)\n",
    "                    transition[state_x][state_y].append(dict(next_state=next_state,reward=reward))\n",
    "        return transition\n",
    "                    \n",
    "    def step(self, action, speed=-1):\n",
    "        # assume we use the max speed as the default speed, when come near to the opt-position, we can slow down the speed\n",
    "        if speed < 0 or speed >= self.UAV_speed:\n",
    "            speed = self.UAV_speed\n",
    "            \n",
    "        self.UAV_path.append(self.UAV_current_pos)\n",
    "        self.UAV_current_pos = self.transition_dynamics(action, speed, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        return self.UAV_current_pos, self.get_reward(self.UAV_current_pos), done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        return choice(self.action_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        return self.UAV_current_pos\n",
    "        \n",
    "    def print_attribute(self):\n",
    "        attrs = vars(self)\n",
    "        print(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "        \n",
    "    def print_locations(self):\n",
    "        print(\"UAV position is: {}\".format(self.UAV_current_pos))\n",
    "        print(\"Users position are: {}\".format(self.users_pos))\n",
    "        \n",
    "    def print_map(self):\n",
    "        x_list = [pos[0] for pos in self.users_pos]\n",
    "        y_list = [pos[1] for pos in self.users_pos]\n",
    "        x_list.append(self.UAV_current_pos[0])\n",
    "        y_list.append(self.UAV_current_pos[1])\n",
    "        for i in range(0, len(self.UAV_path)):\n",
    "            x_list.append(self.UAV_path[i][0])\n",
    "            y_list.append(self.UAV_path[i][1])\n",
    "        \n",
    "        colors = np.array([\"red\", \"green\", \"blue\"])\n",
    "        sizes = []\n",
    "        colors_map = []\n",
    "        for i in range(0, self.number_of_user):\n",
    "            sizes.append(25)\n",
    "            colors_map.append(1)\n",
    "        sizes.append(50)\n",
    "        colors_map.append(0)\n",
    "        for i in range(0, len(self.UAV_path)):\n",
    "            sizes.append(10)\n",
    "            colors_map.append(2)\n",
    "        for i in range(0, len(self.UAV_path) - 1):\n",
    "            x_values = [self.UAV_path[i][0], self.UAV_path[i+1][0]]\n",
    "            y_values = [self.UAV_path[i][1], self.UAV_path[i+1][1]]\n",
    "            plt.plot(x_values, y_values, 'b')\n",
    "        plt.scatter(x_list, y_list, c=colors[colors_map], s=sizes) \n",
    "        plt.axis([0, self.map[\"width\"], 0, self.map[\"length\"]])\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(policy, config, num_episodes=1, render=False):\n",
    "    env = UAVEnvironment(config)\n",
    "    \n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        # all policy will return a direction and a speed\n",
    "        act_direction, act_speed = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(act_direction, act_speed)\n",
    "            act_direction, act_speed = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            if render == True:\n",
    "                clear_output(wait=True)\n",
    "                env.print_attribute()\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Policy choice direction: {}, speed: {}\".format(act_direction, act_speed))\n",
    "                print(\"UAV current position x: {}, y: {}\".format(env.UAV_current_pos[0], env.UAV_current_pos[1]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Class for all RL algorithm\n",
    "class UAVTrainer: \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = UAVEnvironment(self.config)\n",
    "\n",
    "# Value Iteration, Tabular, transition dynamic is known, assume only use fixed speed to reduce action space\n",
    "class UAVTrainerValueIteration(UAVTrainer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.transitions = self.env.get_transition()\n",
    "        self.q_table = []\n",
    "        self.obs_dim = (int(self.env.map[\"width\"] / self.env.UAV_speed), int(self.env.map[\"length\"] / self.env.UAV_speed))\n",
    "        self.act_dim = len(self.env.action_space)\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        \n",
    "        for x in range(0, self.obs_dim[0]+1):\n",
    "            self.q_table.append(list())\n",
    "            for y in range(0, self.obs_dim[1]+1):\n",
    "                self.q_table[x].append(0)\n",
    "            \n",
    "    def get_transition(self, state, act):\n",
    "        transition = self.transitions[state[0]][state[1]][act]\n",
    "        return transition[\"next_state\"], transition[\"reward\"]\n",
    "    \n",
    "    def print_transitions(self):\n",
    "        print(\"Transition width {}, length {}, number of act {}\".format(len(self.transitions), len(self.transitions[0]), len(self.transitions[0][0])))\n",
    "        print(self.transitions)\n",
    "        \n",
    "    def print_table(self):\n",
    "        for j in range(len(self.q_table[0])-1, -1, -1):\n",
    "            for i in range(0, len(self.q_table)):\n",
    "                print(self.q_table[i][j], end =\" \")\n",
    "            print(\"\")\n",
    "            \n",
    "            \n",
    "    def copy_current_table(self):\n",
    "        old_table = []\n",
    "        for x in range(0, self.obs_dim[0]+1):\n",
    "            old_table.append(list())\n",
    "            for y in range(0, self.obs_dim[1]+1):\n",
    "                old_table[x].append(self.q_table[x][y])\n",
    "        return old_table\n",
    "\n",
    "    def update_value_function(self):\n",
    "        old_table = self.copy_current_table()\n",
    "        for state_x in range(self.obs_dim[0] + 1):\n",
    "            for state_y in range(self.obs_dim[1] + 1):\n",
    "                state_value = 0\n",
    "                state_action_values = [0 for i in range(0, self.act_dim)]\n",
    "\n",
    "                for act in range(self.act_dim):\n",
    "                    next_state, reward = self.get_transition((state_x, state_y), act)\n",
    "                    table_x = int(next_state[0] / self.env.UAV_speed)\n",
    "                    table_y = int(next_state[1] / self.env.UAV_speed)\n",
    "                    #print(table_x, table_y)\n",
    "                    state_action_values[act] = state_action_values[act] + reward + self.gamma * old_table[table_x][table_y]   \n",
    "                state_value = np.max(state_action_values)\n",
    "                self.q_table[state_x][state_y] = state_value\n",
    "                #print(\"Update x: {}, y: {} to value {}\".format(state_x, state_y, state_value))\n",
    "            \n",
    "    def train(self):\n",
    "        old_state_value_table = self.copy_current_table()\n",
    "        current_step = 0\n",
    "        while current_step < self.config['max_iteration']:  \n",
    "            current_step = current_step + 1\n",
    "            self.update_value_function()\n",
    "            if current_step % self.config[\"evaluate_interval\"] == 0:\n",
    "                print(\"Iteration {}, Mean Reward is: {}\".format(current_step, evaluate(self.policy, config = self.config, num_episodes=1, render=False)))\n",
    "                #print(\"Iteration {}, Mean Reward is: {}\".format(current_step, 0))\n",
    "                # check exist\n",
    "                stop = True\n",
    "                flag = 0\n",
    "                for x in range(self.obs_dim[0] + 1):\n",
    "                    for y in range(self.obs_dim[1] + 1):\n",
    "                        if abs(self.q_table[x][y] - old_state_value_table[x][y]) > self.config[\"return_threshold\"]:\n",
    "                            stop = False\n",
    "                            flag = 1\n",
    "                    if flag == 1:\n",
    "                        break\n",
    "                if stop == True:\n",
    "                    print(\"Train converge at i = {}\".format(current_step))\n",
    "                    current_step = self.config['max_iteration']\n",
    "                else:\n",
    "                    old_state_value_table = self.copy_current_table()\n",
    "\n",
    "    def policy(self, obs):\n",
    "        table_x = int(obs[0] / self.env.UAV_speed)\n",
    "        table_y = int(obs[1] / self.env.UAV_speed)\n",
    "        next_state_value_list = []\n",
    "        for act in range(0, self.act_dim):\n",
    "            next_state, reward = self.get_transition((table_x, table_y), act)\n",
    "            next_state_x = int(next_state[0] / self.env.UAV_speed)\n",
    "            next_state_y = int(next_state[1] / self.env.UAV_speed)\n",
    "            next_state_value_list.append(self.q_table[next_state_x][next_state_y])\n",
    "        act = np.argmax(next_state_value_list)\n",
    "        return act, self.env.UAV_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: (0, 1, 2, 3), total_steps: 50, current_step: 49, random_seed: 10, map: {'width': 1000, 'length': 1000, 'height': 300}, UAV_speed: 50, UAV_initial_pos: (0, 0), UAV_current_pos: (500, 450), number_of_user: 10, users_pos: [(585, 33), (439, 494), (591, 15), (211, 473), (832, 503), (843, 284), (669, 830), (164, 35), (533, 501), (335, 77)], UAV_path: [(0, 0), (50, 0), (100, 0), (150, 0), (200, 0), (200, 50), (250, 50), (300, 50), (300, 100), (350, 100), (400, 100), (450, 100), (450, 150), (450, 200), (450, 250), (450, 300), (450, 350), (450, 400), (450, 450), (500, 450), (500, 500), (500, 450), (500, 500), (500, 450), (500, 500), (500, 450), (500, 500), (500, 450), (500, 500), (500, 450), (500, 500), (500, 450), (500, 500), (500, 450), (500, 500), (500, 450), (500, 500), (500, 450), (500, 500), (500, 450), (500, 500), (500, 450), (500, 500), (500, 450), (500, 500), (500, 450), (500, 500), (500, 450), (500, 500)], g0: 1e-05, B: 1000000, Pk: 0.1, noise: 1e-09\n",
      "Current Step: 49\n",
      "Policy choice direction: 0, speed: 50\n",
      "UAV current position x: 500, y: 450\n",
      "Current step reward: 0.007704936773218082, episodes rewards: 0.3578324323363084\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD8CAYAAACCRVh7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAchUlEQVR4nO3deXQVdZ738fc3Kwk7GBYBRQ4IOtoiRgV3pMcFRXnUPo89Pg2t9GEcnVanPe1oP3Omn24Hp/uM23Q/LcJxw6XVllFhaKZ9EEGHY4OGFhFBJaJgFDEICTvZvs8fVSEJpEjuvUnqJvfzOqfOrfrVr6p+t6jwubWbuyMiItKcrLgbICIi6UshISIikRQSIiISSSEhIiKRFBIiIhJJISEiIpFaDAkze8LMvjGzdY3K+pnZEjPbGH72DcvNzH5jZqVmttbMxjWaZnpYf6OZTW+fryMiIm2pNXsSTwGXHVZ2N7DU3UcBS8NhgMuBUWE3E5gNQagAPwfOBs4Cfl4fLCIikr5aDAl3fwvYcVjx1cC8sH8eMLVR+dMeWAn0MbPBwKXAEnff4e47gSUcGTwiIpJmcpKcbqC7bwVw961mNiAsHwJ80aheWVgWVX4EM5tJsBdC9+7dzxgzZkySTRQRyUyrV6/e7u5FbTGvZEMiijVT5kcpP7LQfS4wF6C4uNhLSkrarnUiIhnAzDa31bySvbppW3gYifDzm7C8DBjWqN5Q4KujlIuISBpLNiQWAvVXKE0HFjQqnxZe5TQeqAwPS70GXGJmfcMT1peEZSIiksZaPNxkZs8DFwHHmFkZwVVKvwL+YGYzgC3A98Lqi4HJQCmwD7gRwN13mNm9wLthvV+6++Enw0VEJM1YOj8qXOckREQSZ2ar3b24LealO65FRCSSQkJERCIpJEREJJJCQkREIikkREQkkkJCREQiKSRERCSSQkJERCIpJEREJJJCQkREIikkREQkkkJCREQiKSRERCSSQkJERCIpJEREJJJCQkREIikkREQkkkJCREQiKSRERCSSQkJERCIpJEREJJJCQkREIikkREQkkkJCREQiKSRERCSSQkJERCIpJEREJJJCQkREIikkRKSJzRWbueNPd3DF769gTskcqmur426SxCgn7gaISPr4vOJzxj46ln3V+6iuq2b558v548Y/svD7C+NumsREexIicsj9b9/P3uq9VNcFew/7qvfx+qbXWV++PuaWSVwUEiJyyMfbP6amrqZJWW52Lp9XfB5PgyR2CgkROeSq0VdRmFPYpKyqtorxQ8fH1CKJm0JCRA6ZecZMzhl2DoW5hfTK70W3nG7MuWIO/Qr6xd00iYlOXIvIIfk5+SyZtoQ1X69hS+UWzhl2DscUHhN3syRGKe1JmNk/mNmHZrbOzJ43s25mdoKZrTKzjWb2opnlhXXzw+HScPzwtvgCItL2xg4ay1Wjr1JASPIhYWZDgNuAYnc/BcgGrgd+DTzk7qOAncCMcJIZwE53Hwk8FNYTEZE0luo5iRygwMxygEJgK3AxMD8cPw+YGvZfHQ4Tjp9kZpbi8kVEpB0lHRLu/iVwP7CFIBwqgdVAhbvXX0NXBgwJ+4cAX4TT1oT1+x8+XzObaWYlZlZSXl6ebPNERKQNpHK4qS/B3sEJwLFAd+DyZqp6/SRHGddQ4D7X3YvdvbioqCjZ5omISBtI5XDTd4HP3L3c3auBl4FzgD7h4SeAocBXYX8ZMAwgHN8b2JHC8kVEpJ2lEhJbgPFmVhieW5gErAeWAdeFdaYDC8L+heEw4fg33P2IPQkREUkfqZyTWEVwAvovwAfhvOYC/wj8xMxKCc45PB5O8jjQPyz/CXB3Cu0WEZEOYOn8Y764uNhLSkriboaISKdiZqvdvbgt5qXHcoiISCSFhIiIRFJIiIhIJIWEiIhEUkiIiEgkhYSIiERSSIiISCSFhIiIRFJIiIhIJIWEiIhEUkiIiEgkhYSIiERSSIiISCSFhIiIRFJIiIhIJIWEiIhEUkiIiEgkhYSIiERSSIiISCSFhIiIRFJIiIhIJIWEiIhEUkiIiEgkhYSIiERSSIiISCSFhIiIRFJIiIhIJIWEiIhEUkiIiEgkhYSIiERSSIiISCSFhIiIRFJIiIhIJIWEiIhESikkzKyPmc03s4/MbIOZTTCzfma2xMw2hp99w7pmZr8xs1IzW2tm49rmK4iISHtJdU/i34E/ufsY4DRgA3A3sNTdRwFLw2GAy4FRYTcTmJ3iskVEpJ0lHRJm1gu4AHgcwN2r3L0CuBqYF1abB0wN+68GnvbASqCPmQ1OuuUiItLuUtmTGAGUA0+a2Xtm9piZdQcGuvtWgPBzQFh/CPBFo+nLwrImzGymmZWYWUl5eXkKzRMRkVSlEhI5wDhgtrufDuyl4dBSc6yZMj+iwH2uuxe7e3FRUVEKzRMRkVSlEhJlQJm7rwqH5xOExrb6w0jh5zeN6g9rNP1Q4KsUli8iIu0s6ZBw96+BL8xsdFg0CVgPLASmh2XTgQVh/0JgWniV03igsv6wlIiIpKecFKf/MfCcmeUBm4AbCYLnD2Y2A9gCfC+suxiYDJQC+8K6IiKSxlIKCXdfAxQ3M2pSM3UduDWV5YmISMfSHdciIhJJISEiIpEUEiIiEkkhISIikRQSIiISSSEhIiKRFBIiIhJJISEiIpEUEiIiEkkhISIikRQSIiISSSEhIiKRUn0KrIgchbtTtquM3t160yu/V9zNyXjvfPkOL657kZ75Pbnp9Js4rvdxcTcp7SkkRNrJB9s+YOoLU9m6Zyt1XscPx/6Q303+HdlZ2XE3LSM9+u6j3LnkTvZX7yc3O5cH/vwAy6cv54xjz4i7aWlNh5ukS9u2Zxu/fPOXTHtlGi99+BJ1XpfS/Corwayh69ev+Xq1dbVc+uylbKrYxP6a/RysPcgza5/h0ZJHU1q+JKeqtoqfvv5T9lXvw3GqaqvYU7WHO//fnXE3Le1pT0K6rLJdZYx9dCx7qvZwsPYgL294mQUfL+DZa55Nep59+zYd3rmz+Xprt61ld9XuJmX7qvfx5JonufUsvValo5XvLae2rvaI8g3bN8TQms5FexLSZT3w9gPsOriLg7UHAdhbvZeXN7zMxm83Jj1P99bV65nfs9n/lPp065P0siV5g3oMonte9yZlWZbFhKETYmpR56GQkC5r7ba1VNdVNynLy86jdEdp0vOcP7/pcFbEX9DIfiM589gzyc/OP1RWmFvIPefdk/SyJXnZWdnMmzqPgpwCuud2p2deT/oX9OfBSx+Mu2lpT4ebOqHq2uA/vtzs3Jhbkt4uG3kZfy77M/tr9h8qO1BzgOJjm3vjbutcey1s3gzHHx8M1x65s3DIor9ZxM/e+BmvbHiFAd0HcO/Ee5k04og3+0oHmTxqMptu38SiTxbRM68nU0ZPoTC3MO5mpT3z1u4/x6C4uNhLSkribkba2H1wNzcuuJEFHy/AMK4/5XrmXDmHgtyCuJuWlvZV7+PCpy7ko+0fAVBTV8P9f31/m5wTMAs+0/jPRzKYma129+R/DTWiPYlOZMbCGSz6ZBE1dTUAvLT+JQpzC3n0Sl0x05zC3EJW/WgVb21+iy2VW7ho+EW6Ll4kQdqT6CRq62rpNqvboYCo1yOvB7vv2R0xlbQX7UlIOtOeRAYyM7LsyLOkOaZ/wg7XJBkstmaIdARd3dRJZFkW00+bTkFOw/mHwtxCbi6+OcZWZRh3mDMHjj22oezaa2Hr1vjaJNLO9DO0E/nt5b+lMLeQp9Y8RXZWNjefcTO/mPiLuJvV6ezcGRwu6pPoLQuzZsGvfkXl3r0NZQsXwsqVsH499O7dpu0USQfak+hE8nPyefiyh6m4u4Jv7/qWWZNmkZOlnE/EfffBoEEwcCA8mMgl8rt3w333sWLvCPrQ8GiPnJo9Qeo89ljbN1YkDejEtWSMvXuDx2pUh/fXmcH55zechD6qHTtg/Ye8WXteWFA/keNkwVlnwapV7dBqkcS15Ylr7UlIxsjKamUgNKfZCRv9wMrWk12la9KxCskYBQUwezb86EfB8DPPwA03tHLi/d1gwGQq9+wJDzcFAdGDCigshGnT2qXNInHTnoRklJtuggsuCLpWBwQECXP//fQuLMTJYi3DcbLYnT8YjjtOISFdlkJCpLX+9m/hhRfgtNM4NfvL4Gqmv/u74OqmQj0DSLomHW4SScSUKUEnkiG0JyEiIpEUEiIiEkkhISIikVIOCTPLNrP3zGxROHyCma0ys41m9qKZ5YXl+eFwaTh+eKrLFkmUO+zZE9xYJyIta4s9iduBxm8T/zXwkLuPAnYCM8LyGcBOdx8JPBTWE+lQM2fCe+/BX/4Ct90Wd2tE0l9KIWFmQ4ErgMfCYQMuBurfBDwPmBr2Xx0OE46fFNYX6RC7dsG8eVBXF3SPPAIHDsTdKpH0luqexMPAXXDoiWf9gQp3r38zThkwJOwfAnwBEI6vDOs3YWYzzazEzErKy8tTbJ5Ig27dILfRa8ELCpoOi8iRkg4JM7sS+MbdVzcubqaqt2JcQ4H7XHcvdvfioqKiZJsncoS8PHjlFcjPD7qFC/XIJZGWpHIz3bnAVWY2GegG9CLYs+hjZjnh3sJQ4KuwfhkwDCgzsxygN7AjheWLJOySS2D8+KB/4sR42yLSGSS9J+Hu97j7UHcfDlwPvOHuNwDLgOvCatOBBWH/wnCYcPwbns7PKRcRkXa5T+IfgZ+YWSnBOYfHw/LHgf5h+U+Au9th2SIi0oba5NlN7r4cWB72bwLOaqbOAeB7bbE8ERHpGLrjWkREIikkREQkkh4VLhmlpga2bw/eRlpbq0tgRVqiPQnJGO5wxRWwYQOsXw/XXBN3i0TSn0JCMkZlJSxb1vBYjj/+Efbvj7tVIulNISEZo0ePoKvXr1/wqA4RiaaQkIyRkwNvvBG8mrp376Bfj5gUOTqduJaMMnZs0AGcckq8bRHpDLQnISIikRQSIiISSSEhIiKRFBIiIhJJISEiIpF0dZNklF27oKws6N+zp+l9EyJyJO1JSMZwh/PPh02bgm7SpLhbJJL+FBKSMSoqguc2uQfd6tV6LIdISxQSkjF694aBAxuGjztOj+XIZGu3rWXC4xMomFXAqY+cypufvxl3k9KSQkIyRlYWvP02DBoEgwfDihV6LEemqjxQyQVPXsDKspUcqDnAuvJ1TP79ZD7d8WncTUs7CgnJKMOGwejRcOKJcOyxcbdG4vLqR69S67VNyqprq3n6/adjalH6UkiISMaprqvG3ZuU1Xkd1XXVMbUofSkkRCTjXD366iPK8rLzuOHUG2JoTXpTSIhIxinqXsTiGxZzQp8TyLIsBnYfyDP/4xn+asBfxd20tKOb6UQkI11w/AV8etunHKw9SH52PqarGJqlkBCRjGVmdMvRddBHo5CQjFJWBp98EvRv3RpcCisi0XROQjJGXR2cc04QDlu3wnnnBXdei0g0hYRkjMpK+PrrhuHNm+HAgfjaI9IZKCQkY/TpA2PGBHdZm8Hpp0NBQdytEklvOichGcMseBTH6acH/cuWxd0ikfSnkJCM0qtX8GgO0LskRFpDh5tERCSSQkJERCIpJEREJJJCQkREIiUdEmY2zMyWmdkGM/vQzG4Py/uZ2RIz2xh+9g3Lzcx+Y2alZrbWzMa11ZcQEZH2kcqeRA1wp7ufBIwHbjWzk4G7gaXuPgpYGg4DXA6MCruZwOwUli2SlPffhzVrgs8PP4y7NSLpL+mQcPet7v6XsH83sAEYAlwNzAurzQOmhv1XA097YCXQx8z05BzpMDU1MHFicOd1RUXQr8dyiBxdm5yTMLPhwOnAKmCgu2+FIEiAAWG1IcAXjSYrC8sOn9dMMysxs5Ly8vK2aJ4IAHv2BF29HTv0WA6RlqQcEmbWA/gP4A5333W0qs2UHfE7zt3nunuxuxcXFRWl2jyRQ3r3hgsvhKysoJs8WY/lEGlJSndcm1kuQUA85+4vh8XbzGywu28NDyd9E5aXAcMaTT4U+CqV5YskwgwWLw4eywHw8stHry8iqV3dZMDjwAZ3f7DRqIXA9LB/OrCgUfm08Cqn8UBl/WEpkY6SmwvHHBN0OXoojUiLUvkzORf4AfCBma0Jy34G/Ar4g5nNALYA3wvHLQYmA6XAPuDGFJYtIiIdIOmQcPcVNH+eAWBSM/UduDXZ5YmISMfTHdciIhJJISEiIpEUEiIiEkkhIRllyRJYuRJWrYLly+NujUj6U0hIxqiqgqlT4eDB4E7rKVOgri7uVomkN4WEZIwDB6C6umF4//4gOEQkmkJCMkavXvCDHzQ8luPmm6Fbt7hbJZLedM+pZJTHHgseEw7w29/G2xaRzkAhIRnFDHr0aOgXkaPT4SYREYmkkBARkUgKCRERiaSQEBGRSAoJabW2uPEsXeYhIq2jkJAWucMtt0BeHgwcCO+9l/g8KivhrLOCF/2ceWYwnKj33w+Wn5sb3OPgR7z8tmVPPglvvRV0v/994tOLZBrzZP7SOkhxcbGXlJTE3YyM9/bbcMklsHdvMNyjB5xxRmLz2Lw56Oo3t+OPh+HDE5vH6tWwZ0/Qn5UF3/lO8N7q1qqrgxUrGtqQnx/MT2+ok67GzFa7e3FbzEt7EtKiqqqm9xQkc7inrq7pL/9k53G04ZYc/nuotlaHrkRaot9Q0qLzz4cLL4TFi4Ow+M//hIsvTmweX38NZ58N5eXQv3/wFNbBgxObx/LlcOWVUFMDEyfCokWQnZ3YPP7lX+Dee4P+WbOCQ2giEk2Hm6RV3OHcc4P/lP/7v49ed1XZKm5ZfAsfbf+IUwacwuwrZjNu8DhqaoKwGDQo+UM8+/ZBRUUQMMneMf3tt8G0/folN71IumvLw03ak5BWMWvdr+4vd33JpKcnsbc6OIHxzpfvcNFTF/HpbZ9S1L2IoUNTa0dhYdClon//1KYXySQ6JyFt6oV1L1BTV9OkrNZrmb9+fkwtEpFUKCSkTVXXVVPnTc8G19XVUVWrFzeIdEYKCWlT1518HTlZTY9imhnXnHRNTC0SkVQoJKRNjew3kheve5FBPQaRbdkM6TmEV/7nKwzrPSzupolIEnTiuhPYsye4Iue445K/omf79uCegAEDkpvePXg3dGuuSpoyegpfnfgV+6r3UZhbiOnFDSKdlvYk0tyKFcEloyedBBdckNw7mf/t32DIkCBk/vmfE5++thYuvzy4t+Htt2HJkpanMTO653VXQIh0crpPIs2ddRa8+27Qn5UFJ5+c2CWc7sFziuqZNdzv0FoVFfDBBw13J48ZAxs2tH56EelYeixHJ7b88+VMnDeRMf93DP/0xj+xv3r/Uesffk9AVhL/Yof/mE/0x/3hy0z1PgVJTE1dDa+VvsbzHzzP9n3b426OZBidk+hAK7asYPJzk9lfEwTDA39+gHe/epfX/tdrkdPMmQOnnRacD/jhD+GxxxL/T37+/GDaujp49FGYNi2x6d3hjjvgkUegqCh4kqp0jPK95Ux4fALf7P0GCALjxeteZMroKTG3TDKFDjd1oMnPTea/Sv+rSVlBTgHrblnHiL4jIqe76KLgc/ny5Jdd/8+cyikC99Sml8TdvOhmnnjvCarrqg+V9crvRflPy8nL1oOnpHk63NRJ1f8abCwnK4cd+3e0+7LNUv8PXgHR8V7f9HqTgABwdzZ+uzGmFkmmUUh0oBtOvYGCnIImZfk5+YwdNDamFkm6G3PMmCPK9lfv52DtwRhaI5lIIdGBfnz2j5k6Zir52fl0z+3OwO4DWfw3i4+4Q1mk3n2T7qNHXg9yrGEbcZxznziXJ9/TySFpfzonEYNte7bx7f5vGd1/NNlZLV+L2hbnJKTz2rRzE1Oen8KG8g04DX+vhbmFlP+0nMJcXW4mTemcRCc3sMdATi46uVUBITKi7wiqaquaBARAtmXz6Y5PY2qVZIoODwkzu8zMPjazUjO7+2h1d+9ueK9yMtasgZUrk39FZVVV8IKdjz5Kvg07dsCyZbBtW/LzOHAguKFt/9FvqZAubNzgcWRZ0z/XmroahvcZHk+DJGN0aEiYWTbwO+By4GTg+2Z2clT90tLgDuOKisSXddddwZ3F3/0uXHvtke83bklVFZxzDlxxBYwbF9yvkKjPPoORI2Hq1OBzzZrE5/Hqq8Ed1+vWwXe+EwSnZJ5ZF8+iV34vCnIKyCKLwtxC7p14Lz3ze8bdNOniOvSchJlNAP6Pu18aDt8D4O7/2nz9Ys/KKuHEE2HgwMSW9eabjecTvF85P7/101dWwtq1DXsheXkwYUJibfj8c9i8uWF40CAYPTqxeZSUNOxN9egBTz0VhJ5knu37tvPs2mcp31vO1DFTOXPImXE3SdJUW56T6OiQuA64zN1/FA7/ADjb3f++UZ2ZwMxw8BRgXYc1ML0dA+iZDAGtiwZaFw20LhqMdvc22c3s6Gsvm7sdq0lKuftcYC6AmZW0VRp2dloXDbQuGmhdNNC6aGBmbXZZaEefuC4DGr99ZijwVQe3QUREWqmjQ+JdYJSZnWBmecD1wMIOboOIiLRShx5ucvcaM/t74DUgG3jC3T88yiRzO6ZlnYLWRQOtiwZaFw20Lhq02bpI6zuuRUQkXrrjWkREIikkREQkUtqGRCKP7+gKzGyYmS0zsw1m9qGZ3R6W9zOzJWa2MfzsG5abmf0mXD9rzWxcvN+gbZlZtpm9Z2aLwuETzGxVuB5eDC98wMzyw+HScPzwONvdHsysj5nNN7OPwu1jQiZuF2b2D+Hfxjoze97MumXSdmFmT5jZN2a2rlFZwtuBmU0P6280s+ktLTctQyLRx3d0ETXAne5+EjAeuDX8zncDS919FLA0HIZg3YwKu5nA7I5vcru6HdjQaPjXwEPhetgJzAjLZwA73X0k8FBYr6v5d+BP7j4GOI1gvWTUdmFmQ4DbgGJ3P4Xgwpfryazt4ingssPKEtoOzKwf8HPgbOAs4Of1wRLJ3dOuAyYArzUavge4J+52dfA6WAD8NfAxMDgsGwx8HPbPAb7fqP6hep29I7h/ZilwMbCI4CbM7UDO4dsHwZVyE8L+nLCexf0d2nBd9AI+O/w7Zdp2AQwBvgD6hf/Oi4BLM227AIYD65LdDoDvA3MalTep11yXlnsSNGwQ9crCsowQ7hqfDqwCBrr7VoDwc0BYrSuvo4eBu4D65/f2ByrcvSYcbvxdD62HcHxlWL+rGAGUA0+Gh98eM7PuZNh24e5fAvcDW4CtBP/Oq8nc7aJeottBwttHuoZEi4/v6KrMrAfwH8Ad7r7raFWbKev068jMrgS+cffVjYubqeqtGNcV5ADjgNnufjqwl4ZDCs3pkusjPCRyNXACcCzQneCQyuEyZbtoSdT3T3i9pGtIZOTjO8wslyAgnnP3l8PibWY2OBw/GPgmLO+q6+hc4Coz+xx4geCQ08NAH7ND7/Bs/F0PrYdwfG9gR0c2uJ2VAWXuviocnk8QGpm2XXwX+Mzdy929GngZOIfM3S7qJbodJLx9pGtIZNzjO8zMgMeBDe7+YKNRC4H6KxCmE5yrqC+fFl7FMB6orN/t7Mzc/R53H+ruwwn+3d9w9xuAZcB1YbXD10P9+rkurN9lfjG6+9fAF2ZW/5D5ScB6Mmy7IDjMNN7MCsO/lfr1kJHbRSOJbgevAZeYWd9w7+ySsCxa3CdijnKCZjLwCfAp8L/jbk8HfN/zCHb71gJrwm4ywXHUpcDG8LNfWN8IrgD7FPiA4KqP2L9HG6+Ti4BFYf8I4B2gFHgJyA/Lu4XDpeH4EXG3ux3Ww1igJNw2XgX6ZuJ2AfwC+Ijg9QHPAPmZtF0AzxOcj6km2COYkcx2ANwUrpdS4MaWlqvHcoiISKR0PdwkIiJpQCEhIiKRFBIiIhJJISEiIpEUEiIiEkkhISIikRQSIiIS6f8DdioMaU+cUwQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward is: 0.36540047424895894\n"
     ]
    }
   ],
   "source": [
    "environment_config = dict(\n",
    "    total_steps = 50,\n",
    "    random_seed = 10,\n",
    "    is_random_env = False,\n",
    "    map=dict(\n",
    "        width=1000,\n",
    "        length=1000,\n",
    "        height=300\n",
    "    ),\n",
    "    number_of_user = 10,\n",
    "    UAV_speed = 50,\n",
    "    UAV_initial_pos = (0, 0),\n",
    "    wireless_parameter = dict(\n",
    "        g0 = 10 ** (-5),\n",
    "        B = 10 ** (6),\n",
    "        Pk = 0.1,\n",
    "        noise = 10 ** (-9)\n",
    "    )\n",
    ")\n",
    "\n",
    "value_iteration_config = merge_config(dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=100,  # don't need to update policy each iteration\n",
    "    gamma=0.9,\n",
    "    return_threshold=1,\n",
    "), environment_config)\n",
    "trainer = UAVTrainerValueIteration(value_iteration_config)\n",
    "trainer.env.print_locations()\n",
    "trainer.train()\n",
    "\n",
    "print(\"Mean Reward is: {}\".format(evaluate(trainer.policy, config = value_iteration_config, num_episodes=1, render=True))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rest is we try to create more complex environment, with more action space and state, not finished yet, please ignore the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "class UAVEnvironmentComplex(UAVEnvironment):\n",
    "    \"\"\"\n",
    "    Complex UAV environment, action can be compose as (speed, direction), UAV position can be continous float number\n",
    "    \n",
    "    State = (self.UAV_current_pos, self.users_pos [list])\n",
    "    Action = (speed, direction)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(UAVEnvironmentComplex, self).__init__(config)\n",
    "        delattr(self, \"action_space\") \n",
    "\n",
    "\n",
    "    def transition_dynamics(self, action, state):\n",
    "        # action = (speed, direction), speed with 0 and self.UAV_speed, direction with (0, 2 * pi)\n",
    "        speed = action[0]\n",
    "        direction = action[1]\n",
    "        next_x = self.UAV_current_pos[0] + speed * math.cos(direction)\n",
    "        next_y = self.UAV_current_pos[1] + speed * math.sin(direction)\n",
    "        next_x = max(0, next_x)\n",
    "        next_x = min(self.map[\"width\"], next_x)\n",
    "        next_y = max(0, next_y)\n",
    "        next_y = min(self.map[\"length\"], next_y)\n",
    "        return (next_x, next_y)\n",
    "  \n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.UAV_current_pos = self.UAV_initial_pos\n",
    "        return (self.UAV_current_pos, self.users_pos)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # action = (speed, direction)\n",
    "        # This function return the state = (self.UAV_current_pos, self.users_pos [list])\n",
    "        speed = action[0]\n",
    "        speed = max(0, speed)\n",
    "        speed = min(self.UAV_speed, speed)\n",
    "        \n",
    "        standarded_action = (speed, action[1])\n",
    "        self.UAV_current_pos = self.transition_dynamics(standarded_action, self.UAV_current_pos)\n",
    "        self.current_step = self.current_step + 1\n",
    "        done = False\n",
    "        if self.current_step == self.total_steps:\n",
    "            done =  True\n",
    "        state = (self.UAV_current_pos, self.users_pos)\n",
    "        reward = self.get_reward(self.UAV_current_pos)\n",
    "        return state, reward, done\n",
    "    \n",
    "    def action_sample(self):\n",
    "        speed = random.uniform(0, 1) * self.UAV_speed\n",
    "        random_direction = math.pi * 2 * random.uniform(0, 1)\n",
    "        action = (speed, random_direction)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGame = UAVEnvironmentComplex(environment_config)\n",
    "myGame.print_locations()\n",
    "myGame.print_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_complex(policy, config, num_episodes=1, render=False):\n",
    "    env = UAVEnvironmentComplex(config)\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        # all policy will return a direction and a speed\n",
    "        action = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done = env.step(action)\n",
    "            action = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            if render == True:\n",
    "                clear_output(wait=True)\n",
    "                env.print_locations()\n",
    "                print(\"Current Step: {}\".format(env.current_step))\n",
    "                print(\"Policy choice direction: {}, speed: {}\".format((action[1] * 180 / math.pi), action[0]))\n",
    "                print(\"Current step reward: {}, episodes rewards: {}\".format(reward, ep_reward))\n",
    "                env.print_map()\n",
    "                wait(sleep=0.2)\n",
    "        rewards.append(ep_reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Class for all RL algorithm\n",
    "class UAVComplexTrainer: \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = UAVEnvironmentComplex(self.config)\n",
    "            \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def compute_values(self, state):\n",
    "        pass\n",
    "    \n",
    "    def state_to_feature_vector(self, UAV_pos, User_pos_list):\n",
    "        # State (UAV_pos, Users_pos), assmue there is max_number_of_user which UAV can be server, to define the feature vector length\n",
    "        # Feature vector: a vector contain tuple of positions\n",
    "        capacity = self.config[\"max_number_of_user\"] + 1 # UAV + user\n",
    "        features = [0] * capacity * 2\n",
    "        features[0] = UAV_pos[0]\n",
    "        features[1] = UAV_pos[1]\n",
    "        for i in range(0, len(User_pos_list)):\n",
    "            features[2*i+2] = User_pos_list[i][0]\n",
    "            features[2*i+3] = User_pos_list[i][1]\n",
    "        return features\n",
    "\n",
    "    def policy(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from random policy\n",
    "class UAVComplexTrainerRandomPolicy(UAVComplexTrainer):\n",
    "    def __init__(self, config):\n",
    "        super(UAVComplexTrainerRandomPolicy, self).__init__(config)\n",
    "        \n",
    "    def policy(self, obs):\n",
    "        action = self.env.action_sample()\n",
    "        return action\n",
    "\n",
    "random_policy_config = environment_config\n",
    "trainer = UAVComplexTrainerRandomPolicy(random_policy_config)\n",
    "print(\"Mean Reward is: {}\".format(evaluate_complex(trainer.policy, config = random_policy_config, num_episodes=1, render=True))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def to_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        x = np.array(x) \n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.float32)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    return x\n",
    "\n",
    "\n",
    "class NetworkModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(NetworkModel, self).__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_dim,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100,act_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.network(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Class for all RL algorithm\n",
    "class UAVComplexTrainerNN(UAVComplexTrainer): \n",
    "    def __init__(self, config):\n",
    "        super(UAVComplexTrainerNN, self).__init__(config)\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.model = NetworkModel((self.config[\"max_number_of_user\"] + 1) * 2, 2)\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def policy(self, state):\n",
    "        if np.random.uniform(0,1) <= self.config[\"eps\"]:\n",
    "            action = self.env.action_sample()\n",
    "        else:\n",
    "            feature = self.state_to_feature_vector(state[0], state[1])\n",
    "            model_input = to_tensor(feature).squeeze()\n",
    "            action = self.model(model_input)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_function_config = merge_config(dict(\n",
    "    total_steps = 50,\n",
    "    number_of_user = 10,\n",
    "    max_number_of_user = 20,\n",
    "    map=dict(\n",
    "        width=1000,\n",
    "        length=1000,\n",
    "        height=100\n",
    "    ),\n",
    "    max_episode_length=10000,\n",
    "    eps=0.01,\n",
    "    gamma=0.9,\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "), environment_config)\n",
    "\n",
    "NNTrainer = UAVComplexTrainerNN(linear_function_config)\n",
    "print(\"Mean Reward is: {}\".format(evaluate_complex(NNTrainer.policy, config = linear_function_config, num_episodes=1, render=True))) # Enable render=True can see the agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, (10,3))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
